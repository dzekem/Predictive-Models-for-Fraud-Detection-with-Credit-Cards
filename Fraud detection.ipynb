## Project Overview

### Credit Card Fraud Detection

Credit card fraud is a situation where unauthorized users take people's credit card information and try using them for purchases or removing money from the account. This is also classified under Identity theft.  
Fraud detection projects have been more common nowadays since every business owner will want their customers to be protected at it's maximum level. 

#### Project Approach
- Necessary libraries and data import is perform after which the data description is used to understand it and also check for null values
- The target class which is binary is noticed to have imbalanced classes, this is either handled by performing boostrap resampling of the minority class or use a model that is prone to imbalanced data.
- I chose to use a model that handles imbalanced data which are tree based models since they use information gain like Gini impurity or entropy to make a split and coupled with advanced models that have optimization inbuilt during the process of bagging or boosting.

#### Importing Packages

import sys
!{sys.executable} -m pip install xgboost
import pandas as pd 
import numpy as np
import matplotlib
import seaborn as sns
import matplotlib.pyplot as plt
import xgboost as xgb
import gc
from datetime import datetime 
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.metrics import roc_auc_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report


### Data description

data = pd.read_csv('creditcard.csv')
data.head()

data.describe()

data.info()

### Check for Null Data

def check_null_values(df):
    flag=df.isna().sum().any()
    if flag==True:
        total = df.isnull().sum()
        percent = (df.isnull().sum())/(df.isnull().count()*100)
        output = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
        data_type = []
        for col in df.columns:
            dtype = str(df[col].dtype)
            data_type.append(dtype)
        output['Types'] = data_type
        return(np.transpose(output))
    else:
        return(False)

check_null_values(data)

#### Exploratory Data Analysis
Since there are no missing values, the target class is analysed to see if it's biased or not.  
Instances for each class are printed out and visualized which shows the data is highly imbalanced 

data["Class"].value_counts()

# plot a scatter plot of the target class with Amount and time as Y axis and x axis respectively
plt.style.use("ggplot")
sns.FacetGrid(data, hue="Class", size = 6).map(plt.scatter, "Time", "Amount", edgecolor="k").add_legend()
plt.show()

s = sns.boxplot(x="Class", y="Amount", hue="Class",data=data,showfliers=True)
plt.show();

### Observation from the visuals above  
The scatter plot and box plot above clearly shows that all the fraudulent transactions made using credit cards are always lessthan 2500 dollars. This will therefore help better analyse False Positive instances of transactions greater than 2500 dollars

### Feature Correlation Using Heatmap
  
Since the features have been encoded for privacy purposes using PCA (Principal Component Analysis), I will perform feature correlation to see which 
features have a direct and inverse correlation with the target class


plt.figure(figsize = (14,14))
plt.title('Credit Card Transactions features correlation plot (Pearson)')
corr = data.corr()
sns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1)
plt.show()

### Observations from Feature Correlation 

- V7 and V20 is noticed to have high direct correlation with Amount, which implies an increase in these faetures causes an increase in credit card amount
- V2 and V5 has an inverse correlation with Amount, implies an increase in these features causes a decrease in the credit card amount.
- This therefore means high values of these features will lead to a reduced amounts which is where the fraudulent transactions are targeted.

# plot a scatter plot of the target class with Amount and time as Y axis and x axis respectively
plt.style.use("ggplot")
sns.FacetGrid(data, hue="Class", size = 6).map(plt.scatter, "V7", "Amount", edgecolor="k").add_legend()

sns.FacetGrid(data, hue="Class", size = 6).map(plt.scatter, "V20", "Amount", edgecolor="k").add_legend()
plt.show()

# plot a scatter plot of the target class with Amount and time as Y axis and x axis respectively
plt.style.use("ggplot")
sns.FacetGrid(data, hue="Class", size = 6).map(plt.scatter, "V2", "Amount", edgecolor="k").add_legend()

sns.FacetGrid(data, hue="Class", size = 6).map(plt.scatter, "V5", "Amount", edgecolor="k").add_legend()
plt.show()

### Building 2 Predictive Models to pick the most optimal
Here I will be building two tree based models after analysing model performance and robustness, the best will be chosen
- Random Forest
- XGBoost

#### Splitting the Data into Train, Validation and Test set  
The dataset is first splitted into train and test with a ratio of 80:20  
Later split train into validation and final train set in to 80:20

X = data.loc[:,data.columns!='Class']
y = data['Class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size=0.2)


#### Randon Forest Model

- Random forest classifier is a tree based classifier which uses sub-samples of the dataset where accuracy and overfitting is controlled y averaging the results gotten.

Parameters chosen when training this model on the dataset are:
- n_jobs defines the number of jobs that can run in parallel, values is set to 4
- criterion is the function used to measure the quality of the split, here I'll stick on the default which is gini impurity. 
- n_estimators defines the number of trees in the forest, the default value is maintained
- random_state is used to control the randomness of the samples since bootstrap is true by default, this is used to consider when looking for the best split at each node
- verbose controls the verbosity of the model when fitting and predicting, this is set to False 
- Random forest uses bagging where parallel trees are built and the mean of each tree is taken. Bagging is aimed at reducing variance(overfitting) and if not well implemented will likely have high Bias(model underfitting)

clf = RandomForestClassifier(n_jobs=4, random_state=200, criterion='gini', 
                             n_estimators=100, verbose=False)

clf.fit(X_train, y_train)

y_pred_val = clf.predict(X_val)

roc_auc_score(y_val, y_pred_val)

### Feature Importance  
This is the technique used to calculate the score of all the input features to the model. These score can be used in cases like:
- Model interpretation: Model interpretation makes communication and interpretation of the model easier to other teams and stakeholders.
- Model improvement and performance: Feature importannce can be used to build a more robust model by reducing overfitting and dimensionality. Features noticed to have very low scores and little correlation to the target feature can be dropped as a measure of feature engineering
- With tree based models, when implementing feature importance normalization is not needed since the trees are splitted through information gain using measures like Gini impurity and Entropy

feature_imp = pd.DataFrame({'Feature':X , 'Feature importance': clf.feature_importances_})
feature_imp = feature_imp.sort_values(by='Feature importance',ascending=False)
plt.figure(figsize = (8,6))
plt.title('Features importance',fontsize=14)
s = sns.barplot(x='Feature',y='Feature importance',data=feature_imp)
s.set_xticklabels(s.get_xticklabels(),rotation=90)
plt.show()

### Confusion Matrix

A confussion matrix is a table used to measure indepth model performance especially when the target class is imbalanced. Also, it represents the number of instances in the actual class and the predicted class.
Confusion matrix contains:
- True Negative (TN) - Actual instances that are NO and predicted as NO
- False Positive (FP) - Actual instances that are NO and predicted YES
- False Negative (FN) - Actual instances that are YES and predicted as NO
- True Positive (TP) - Actual instances that are YES and predicted as YES

- Precision tells how much your classifier predicted correctly  
precision = TP / TP + FP

- Recall shows how often your classifier predicts YES when it is actually YES.  
Recall = TP / TP + FN

- F1-score is the harmonic mean of precision and recall.  
F1-score = 2*Recall * Precision / Recall + Precision

cm = pd.crosstab(y_val, y_pred_val, rownames=['Actual'], colnames=['Predicted'])
fig, (ax1) = plt.subplots(ncols=1, figsize=(8,8))
sns.heatmap(cm, 
            xticklabels=['Not Fraud', 'Fraud'],
            yticklabels=['Not Fraud', 'Fraud'],
            annot=True,ax=ax1,
            linewidths=.2, cmap="OrRd")
plt.title('Confusion Matrix', fontsize=14)
plt.show()


print("[Validation Classification Report:]")
print(classification_report(y_val, y_pred_val))

### XGBOOST MODEL

- XGBoost also called Extreme Gradient Boosting is a tree based model that uses Gradient Boosting to improve performance and speed. 
- XGBoost uses Boosting where weak learners(smaller decision trees) are sequencially built each learning from the previous thus boosting the model performance. 
- 

### Setting the parameters and building the model  
- Objective: Defines the output format of the model, logistic regression is chosen here.
- eval_metric: This is the evaluation metric for validation data and it is assigned according to the objective chosen, area under the curve is set here.
- subsample: the value set here is used to randomly sample that ratio from training data prior to growing trees, this also prevents overfitting.
- max_depth: this is used to set the maximum tree depth, having large trees will likely cause overfitting thus it is set to 2

MAX_ROUNDS = 1000 #lgb iterations
EARLY_STOP = 50 #lgb early stop 
OPT_ROUNDS = 1000  #To be adjusted based on best validation rounds
VERBOSE_EVAL = 50 #Print out metric result


dtrain = xgb.DMatrix(X_train, y_train)
dvalid = xgb.DMatrix(X_val, y_val)
dtest = xgb.DMatrix(X_test, y_test)

#What to monitor (in this case, **train** and **valid**)
watchlist = [(dtrain, 'train'), (dvalid, 'valid')]


params = {}
params['objective'] = 'reg:logistic'
params['eta'] = 0.039
params['silent'] = True
params['max_depth'] = 2
params['subsample'] = 0.8
params['eval_metric'] = 'auc'
params['random_state'] = 200

model = xgb.train(params, 
                dtrain, 
                MAX_ROUNDS, 
                watchlist, 
                early_stopping_rounds=EARLY_STOP, 
                maximize=True, 
                verbose_eval=VERBOSE_EVAL)


val_pred = model.predict(dvalid)

roc_auc_score(y_val, val_pred )

test_pred = model.predict(dtest)

roc_auc_score(y_test, test_pred)

fig, (ax) = plt.subplots(ncols=1, figsize=(8,5))
xgb.plot_importance(model, height=0.8, title="Features importance (XGBoost)", ax=ax, color="green") 
plt.show()

### Conclussion
- Feature correlation is a very important stage during model training as it helps understand what input variables are related to the target and how.
- It's seen from the visual that V2 and V5 have inverse correlation with amount. This implies increase in these features will lead to reduced amounts of the transaction which is a very important feature to monitor as most of the fraudulent transactions are lessthan 2000 dollars.
- From the model performance above, XGBoost has the best model performance and doesn't overfit. 
- Since the data is imbalanced, accuracy score was therefore not he best way to check model accuracy which is why I used area under the ROC (Receiver Operator Characteristics) curve and F1-score from classification report.
- Feature importance also shows that the top features of greatest importance to this model making correct predictions are V14, V17, V10, V7 and V12.

